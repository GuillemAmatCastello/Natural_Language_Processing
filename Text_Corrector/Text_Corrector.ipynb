{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things left to do:\n",
    "- Finalize Untokenization of words.\n",
    "- Improve word tokenization precision.\n",
    "- Try to parallelize the process.\n",
    "- Finish Tokenizer class.\n",
    "- Try to make the process more efficient: \n",
    "    - Only calculate distance between misspelled word and a subset of the dictionary\n",
    "    - Use numpy for levehnstein distance formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Basic Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build from scratch a spelling corrector in Python. It should include:\n",
    "\n",
    "1. tokenization\n",
    "2. edit distance-based non-word spelling correction\n",
    "3. de-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing packages'''\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading test file\n",
    "text_path = r'C:\\Users\\guill\\Desktop\\Current Semester\\Natural Language Processing\\Homeworks\\Homework_2\\austen-sense-corrupted.txt'\n",
    "#text_path = 'austen-sense-corrupted.txt'\n",
    "\n",
    "with open(text_path, 'r', encoding = 'utf-8') as file:\n",
    "    corrupted_text = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = corrupted_text[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing English dictionary'''\n",
    "dictionary_path = 'dict.txt'\n",
    "\n",
    "with open(dictionary_path, 'r', encoding = 'utf-8') as file:\n",
    "    dictionary_string = file.read()\n",
    "    file.close()\n",
    "\n",
    "dictionary = re.split(r'\\n', dictionary_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improve the regex when time allows\n",
    "def tokenize(text: str) -> list:\n",
    "    text = str(text)\n",
    "    return re.findall(r\"[A-Za-z]+(?:'?[a-z]*)\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_words(tokens: list, dictionary: list) -> list:\n",
    "    incorrect_words = [word.lower()\n",
    "                       for word in tokens\n",
    "                       if (word.lower() not in dictionary)\n",
    "                       and (word not in dictionary)]\n",
    "    return set(incorrect_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_words = wrong_words(list_of_words, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(A: str, B:str) -> int:\n",
    "    '''Returns the Levensthein edit distance between strings A and B. Output is an integer.'''\n",
    "    # Creating variables for lengths of A and B plus 1\n",
    "    n = len(A) + 1\n",
    "    m = len(B) + 1\n",
    "    \n",
    "    # Creating matrix D with edit distances between strings A (rows) and B (columns).\n",
    "    # Row 0 and Column 0 represent empty strings.\n",
    "    D = [[None for j in range(m)] for i in range(n)]\n",
    "    \n",
    "    # Filling out column 0 and row 0 with edit distances equal to i and j respectively\n",
    "    # which is the edit distance between an empty string and the i-th or j-th character.\n",
    "    for i in range(n): D[i][0] = i\n",
    "    for j in range(m): D[0][j] = j\n",
    "    \n",
    "    # Filling out the rest of the matrix with the minimum edit distances\n",
    "    for j in range(1, m):\n",
    "        for i in range(1, n):\n",
    "            ins = D[i][j - 1] + 1     # insertion adds 1\n",
    "            dlt = D[i - 1][j] + 1     # deletion adds 1\n",
    "            mtc = D[i - 1][j - 1]     # match adds 0\n",
    "            mis = D[i - 1][j - 1] + 1 # mismatch adds 1 (substitution)\n",
    "\n",
    "            if A[i - 1] == B[j - 1]: \n",
    "                D[i][j] = min(ins, dlt, mtc)\n",
    "            else:\n",
    "                D[i][j] = min(ins, dlt, mis)\n",
    "    \n",
    "    # Returns optimal distance between two strings\n",
    "    return D[n - 1][m - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests: Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_distance('level', 'level') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_distance('level', 'leaven') == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline: Correct Spelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(W:str, D: dict) -> str:\n",
    "    '''\n",
    "    Returns the correct spelling of a word (a string). \n",
    "    Uses get_distance() to compute distance between string W and each word in a dictionary D (list of strings).\n",
    "    Returns the word in the dictionary D with the minimum distance to W (first appearance).\n",
    "    '''\n",
    "    # Loops through each word d in D to compute get_distance(W, d)\n",
    "    min_distance = float('inf')\n",
    "    min_index    = None\n",
    "    \n",
    "    for i, d in enumerate(D): \n",
    "        distance = get_distance(W, d)\n",
    "        \n",
    "        if distance < min_distance: \n",
    "            min_distance = distance\n",
    "            min_index    = i\n",
    "    \n",
    "    return D[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling_2(W, D):\n",
    "    '''\n",
    "    Returns the correct spelling of a word (a string). \n",
    "    Uses get_distance() to compute distance between string W and each word in a dictionary D (list of strings).\n",
    "    Returns the word in the dictionary D with the minimum distance to W (first appearance).\n",
    "    '''\n",
    "    # Creates empty list of distances between W and each element in D\n",
    "    distances = []\n",
    "    \n",
    "    # Loops through each word d in D to compute get_distance(W, d)\n",
    "    for d in D: distances.append(get_distance(W, d))\n",
    "    \n",
    "    # Retrieves the first word in D with the minimum distance to W\n",
    "    min_distance = min(distances)\n",
    "    min_index    = distances.index(min_distance)\n",
    "    \n",
    "    return D[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_list(L, D):\n",
    "    '''\n",
    "    Returns a list of strings corrected with correct_spelling(), \n",
    "    after checking if the word exists in the dictionary D.\n",
    "    '''\n",
    "    # Loops the words l in list L and corrects the ones that are not found in D\n",
    "    correct_L = []\n",
    "    \n",
    "    for l in L:\n",
    "        if l in D: \n",
    "            correct_L.append(l)\n",
    "        else:\n",
    "            correct_L.append(correct_spelling(l, D))\n",
    "            \n",
    "    return correct_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests: Time Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adad\n",
      "time_elapsed: 9.080358743667603\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "print(correct_spelling('adsad', dictionary))\n",
    "t1 = time.time()\n",
    "\n",
    "time_elapsed = t1 - t0\n",
    "print('time_elapsed:', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adad\n",
      "time_elapsed: 9.220005512237549\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "print(correct_spelling_2('adsad', dictionary))\n",
    "t1 = time.time()\n",
    "\n",
    "time_elapsed = t1 - t0\n",
    "print('time_elapsed:', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispelled_words = ['estres', 'think', 'panicok', 'neturral', 'probability', 'millom', 'he', 'ittt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estre', 'think', 'panic', 'demurral', 'probability', 'billon', 'he', 'Atta']\n",
      "time_elapsed: 58.67059826850891\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "print(correct_list(mispelled_words, dictionary))\n",
    "t1 = time.time()\n",
    "\n",
    "time_elapsed = t1 - t0\n",
    "print('time_elapsed:', time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE Completed by Sebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential list of steps to speed up the process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a dictionary compiled from the book.\n",
    "- Check if a word is correctly spelled. Run through the whole dictionary to see if it exists.\n",
    "- Investigate whether Spark is a possibility.\n",
    "- Assume whether the two first letters are okey.\n",
    "- Filter for misspelled words.\n",
    "- Ignore whatever that starts with a Capital Letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Tokenization:* https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence, https://medium.com/analytics-vidhya/tokenization-building-a-tokenizer-and-a-sentencizer-c19a00393c19\n",
    "- *Levehnstein Distance:* https://www.python-course.eu/levenshtein_distance.php, https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/, https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python,\n",
    "https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Advanced Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If given more time we would have liked to implement an *Advanced Tokenizer* class. The class would establish a hierarchical structure composed of paragraphs, sentences and words(tokens). It would allow the user to identify tokens with higher precision, perform modifications on them and, more importantly, revert back to the text in its original form. The code and the tests below show our work on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    '''Tokenizer class that allows the user\n",
    "    to break text into tokens'''\n",
    "    def __init__(self, text = None, sentences = None):\n",
    "        self.text = str(text) if text is not None else None\n",
    "        self.sentences = sentences if sentences is not None else []\n",
    "        self.paragraphs = []\n",
    "        self.tokens = []\n",
    "        \n",
    "    def sentencize(self):\n",
    "        '''Splits the text into sentences'''\n",
    "        self.paragraphs = [paragraph.replace('\\n', ' ')\n",
    "                           for paragraph in re.split(r'(?:\\n){2,}', self.text)]\n",
    "        \n",
    "        self.sentences = [re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z][a-z]{2}\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "                          for paragraph in self.paragraphs]\n",
    "        \n",
    "        return self.sentences\n",
    "        \n",
    "    def _desentencize(self):\n",
    "        '''Joins sentences into a single text corpus'''\n",
    "        self._paragraphs = [' '.join(sentence) for sentence in self.sentences]\n",
    "        self._text = ['\\n'.join(self._paragraphs)]\n",
    "        return self._text\n",
    "        \n",
    "    def tokenize(self):\n",
    "        '''Splits sentences into words or tokens'''\n",
    "        self.tokens = [re.split(r'\\s', sentence) for sentence in self.sentences]\n",
    "        #if self.paragraphs is None:\n",
    "        #else:\n",
    "        #    self.tokens = [re.split(r'\\s', words) for sentence in self.sentences]\n",
    "        \n",
    "        \n",
    "        return self.tokens\n",
    "        \n",
    "    def detokenize(self):\n",
    "        '''Joins words into sentences'''\n",
    "        self.sentences = [\"\".join([\" \"+i if not i.startswith(\"'\")\n",
    "                                   and i not in string.punctuation\n",
    "                                   else i for i in tokens]).strip()\n",
    "                          for tokens in self.tokens]\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests: Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample objects to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''I worked on my NLP Assignment today. This is another sentence.\\n\n",
    "I tested many methods and models.\\n\n",
    "I think I might have succeeded.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences =  ['Hello my friend, how are you?', 'How is it going?', 'It is 12.30pm', 'Morning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving from text to sentences and paragraphs and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenizer(text = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.sentencize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences._desentencize()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving from sentences to words and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer(sentences = list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.detokenize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
