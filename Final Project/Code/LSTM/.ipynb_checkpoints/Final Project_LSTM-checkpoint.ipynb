{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Guillem Amat (ga98), Sebastián Soriano Pérez (ss1072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular Packages\n",
    "from typing import List, Dict, Tuple\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import requests\n",
    "import pdb\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch Packages\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'OneDrive', 'Desktop', 'Final Project', 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(path, 'transformer', 'imdb5k_train.csv'))\n",
    "test  = pd.read_csv(os.path.join(path, 'transformer', 'imdb5k_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(review:str) -> str:\n",
    "    regex  = re.compile('<*.?>')\n",
    "    review = re.sub(regex, ' ', review)\n",
    "    review = re.sub(' +',  ' ', review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing HTML tags and other issues\n",
    "train['text'] = train['text'].apply(lambda x: clean(x))\n",
    "test['text']  = test['text'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to lower case\n",
    "train['text'] = train['text'].apply(lambda x: x.lower())\n",
    "test['text']  = test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "train['text'] = train['text'].str.replace(f'[{punctuation}]', '')\n",
    "test['text']  = test['text'].str.replace(f'[{punctuation}]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding up train and test set into a single list\n",
    "full = train['text'].tolist() + test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word4review = list(map(word_tokenize, full))\n",
    "words       = [word for review in word4review for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count    = Counter(words)\n",
    "frequent_sort = word_count.most_common(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 135176), ('and', 66068), ('a', 65418), ('of', 58530), ('to', 54693)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting most common words\n",
    "word_count.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {w: i + 1 for i, (w, _) in enumerate(frequent_sort)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding Reviews & Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_review = [[word2int[word] for word in review] for review in word4review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test = encoded_review[:5000], encoded_review[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [1 if label == 'pos' else 0 for label in train['label'].tolist()]\n",
    "y_test  = [1 if label == 'pos' else 0 for label in test['label'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(x) for x in encoded_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Think of a better plot\n",
    "pd.Series(lengths).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean       232.957800\n",
       "std        175.165521\n",
       "min         10.000000\n",
       "25%        125.000000\n",
       "50%        174.000000\n",
       "75%        284.000000\n",
       "max       2459.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lengths).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outlier reviews: [10 words < review < 500 words]\n",
    "train_outliers = [True if len(x)<=500 else False for x in X_train]\n",
    "test_outliers  = [True if len(x)<=500 else False for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train outlier removal\n",
    "X_train = [x for x, y in zip(X_train, train_outliers) if y == True]\n",
    "y_train = [x for x, y in zip(y_train, train_outliers) if y == True]\n",
    "\n",
    "#Test outlier removal\n",
    "X_test = [x for x, y in zip(X_test, test_outliers) if y == True]\n",
    "y_test = [x for x, y in zip(y_test, test_outliers) if y == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data: List[str], seq_length: int) -> List[str]:\n",
    "    \n",
    "    features = np.zeros((len(data), seq_length))\n",
    "    \n",
    "    for i, review in enumerate(data):      \n",
    "        words = len(review)\n",
    "        \n",
    "        if words <= seq_length:\n",
    "            padding    = list(np.zeros(seq_length-words))\n",
    "            new_review = padding + review\n",
    "            \n",
    "        #This step is not necessary with our code, but still nice check    \n",
    "        elif words > seq_length:\n",
    "            new_review = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new_review)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding test and train datasets\n",
    "X_train = pad(X_train, 500)\n",
    "X_test  = pad(X_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test)  == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.utils.data.TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(np.array(y_train)).float())\n",
    "data_test  = torch.utils.data.TensorDataset(torch.from_numpy(X_test).long(), torch.from_numpy(np.array(y_test)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 250\n",
    "train_loader = torch.utils.data.DataLoader(data_train, shuffle=True, batch_size=batch, drop_last=True)\n",
    "test_loader  = torch.utils.data.DataLoader(data_test, shuffle= True, batch_size=batch, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocabulary, n_output, n_embedding, n_hidden, n_layers, drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_output = n_output\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary, n_embedding)\n",
    "        self.lstm      = nn.LSTM(n_embedding, n_hidden, n_layers,\n",
    "                                 dropout=drop, batch_first=True)\n",
    "        \n",
    "        self.linear  = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        #pdb.set_trace()\n",
    "        batch = x.size(0)\n",
    "        embed = self.embedding(x)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        output = self.linear(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        output = output.view(batch, -1)[:, -1]\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(vocabulary, n_output=1, n_embedding=300, n_hidden=256, n_layers=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0.695\n",
      "0.468\n",
      "0.696\n",
      "0.488\n",
      "0.689\n",
      "0.58\n",
      "0.685\n",
      "0.564\n",
      "0.678\n",
      "0.588\n",
      "0.665\n",
      "0.628\n",
      "0.665\n",
      "0.576\n",
      "0.666\n",
      "0.596\n",
      "0.657\n",
      "0.6\n",
      "0.635\n",
      "0.64\n",
      "0.65\n",
      "0.616\n",
      "0.649\n",
      "0.624\n",
      "0.662\n",
      "0.576\n",
      "0.654\n",
      "0.604\n",
      "0.649\n",
      "0.604\n",
      "0.636\n",
      "0.656\n",
      "0.613\n",
      "0.668\n",
      "0.696\n",
      "0.656\n",
      "test\n",
      "0.615\n",
      "0.664\n",
      "0.61\n",
      "0.676\n",
      "0.617\n",
      "0.656\n",
      "0.614\n",
      "0.648\n",
      "0.628\n",
      "0.66\n",
      "0.611\n",
      "0.64\n",
      "0.605\n",
      "0.684\n",
      "0.589\n",
      "0.716\n",
      "0.609\n",
      "0.692\n",
      "0.623\n",
      "0.64\n",
      "0.62\n",
      "0.62\n",
      "0.609\n",
      "0.648\n",
      "0.638\n",
      "0.632\n",
      "0.601\n",
      "0.664\n",
      "0.627\n",
      "0.624\n",
      "0.644\n",
      "0.584\n",
      "0.618\n",
      "0.66\n",
      "0.62\n",
      "0.656\n",
      "---------------------------------------------------\n",
      "| Epoch: 1   | Train Loss: 0.64 | Test Loss: 0.60 |\n",
      "---------------------------------------------------\n",
      "train\n",
      "0.576\n",
      "0.692\n",
      "0.587\n",
      "0.676\n",
      "0.588\n",
      "0.676\n",
      "0.546\n",
      "0.724\n",
      "0.568\n",
      "0.68\n",
      "0.534\n",
      "0.732\n",
      "0.561\n",
      "0.712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-ba377c090a67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m#Calculating loss and backpropagating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mloss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#Clipping Gradients and taking a step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_loss = 0\n",
    "test_loss  = 0\n",
    "train_history  = []\n",
    "test_history   = []\n",
    "train_accuracy = []\n",
    "test_accuracy  = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        test_loss  = 0\n",
    "        hidden = lstm.init_hidden(batch)\n",
    "        \n",
    "        lstm.train()\n",
    "        for _, data in enumerate(train_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            X_train, y_train = data\n",
    "            X_train, y_train = Variable(torch.squeeze(X_train)), Variable(torch.squeeze(y_train))\n",
    "            \n",
    "            #Forward pass\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            y_hat, hidden = lstm(X_train, hidden)\n",
    "            \n",
    "            #Calculating loss and backpropagating\n",
    "            loss  = criterion(y_hat, y_train)\n",
    "            loss.backward()\n",
    "            \n",
    "            #Clipping Gradients and taking a step\n",
    "            nn.utils.clip_grad_norm_(lstm.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Storing loss results\n",
    "            train_loss += loss.item() \n",
    "            train_history.append(loss.item())\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            #Calculating Accuracy\n",
    "            predictions = torch.round(y_hat.squeeze())\n",
    "            num_correct = predictions.eq(y_train.float().view_as(predictions))\n",
    "            correct     = np.squeeze(num_correct.numpy())\n",
    "            accuracy    = np.sum(correct)/batch\n",
    "            train_accuracy.append(accuracy)\n",
    "            \n",
    "            #Printing for result evaluation\n",
    "            #print(round(loss.item(), 3))\n",
    "            #print(accuracy)\n",
    "            \n",
    "        hidden = lstm.init_hidden(batch)\n",
    "        \n",
    "        #print('test')\n",
    "        lstm.eval()\n",
    "        for _, data in enumerate(test_loader):\n",
    "            \n",
    "            X_test, y_test = data\n",
    "            X_test, y_test = torch.squeeze(X_test), torch.squeeze(y_test)\n",
    "            \n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            y_hat, hidden = lstm(X_test, hidden)\n",
    "            \n",
    "            #Checking Loss\n",
    "            loss = criterion(y_hat, y_test)\n",
    "            test_loss += loss.item()\n",
    "            test_history.append(loss.item())\n",
    "            \n",
    "            #Calculating Accuracy\n",
    "            predictions = torch.round(y_hat.squeeze())\n",
    "            num_correct = predictions.eq(y_test.float().view_as(predictions))\n",
    "            correct     = np.squeeze(num_correct.numpy())\n",
    "            accuracy    = np.sum(correct)/batch\n",
    "            test_accuracy.append(accuracy)\n",
    "            \n",
    "            #Printing for result evaluation\n",
    "            #print(round(loss.item(), 3))\n",
    "            #print(accuracy)\n",
    "        \n",
    "        train_loss = str(train_loss*batch/len(train_loader.dataset))[:4]\n",
    "        test_loss  = str(test_loss*batch/len(test_loader.dataset))[:4]\n",
    "        \n",
    "        \n",
    "        string = f'''| Epoch: {epoch + 1}   | Train Loss: {train_loss} | Test Loss: {str(test_loss)[:4]} |'''\n",
    "        print('-'*len(string)); print(string); print('-'*len(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting train and test loss'''\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.title('LSTM Train & Test Loss')\n",
    "plt.plot(train_history, color='#000E43', label = 'Train Loss')\n",
    "plt.plot(test_history, color='#22906F', label = 'Test Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting train and test accuracy'''\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.title('LSTM Train & Test Accuracy')\n",
    "plt.plot(train_accuracy, color='#000E43', label = 'Train Loss')\n",
    "plt.plot(test_accuracy, color='#22906F', label = 'Test Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary Review Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_dim_pad(review: List[str], seq_length:int = 500) -> np.array:\n",
    "    length = len(review)\n",
    "    \n",
    "    if length <= seq_length:\n",
    "            padding    = list(np.zeros(seq_length-length))\n",
    "            new_review = padding + review\n",
    "             \n",
    "    elif length > seq_length:\n",
    "        new_review = review[0:seq_length]\n",
    "        \n",
    "    new_review = np.array(new_review)\n",
    "    \n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(review: str, word_dictionary: Dict[str, int]) -> np.array:\n",
    "    '''Cleans & Preprocesses a single string for the LSTM'''\n",
    "    review = review.lower()\n",
    "    review = ''.join([ch for ch in review if ch not in string.punctuation])\n",
    "    review = word_tokenize(review)\n",
    "    review = [word_dictionary[word] for word in review]\n",
    "    review = one_dim_pad(review, 500)\n",
    "    return torch.Tensor(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test  = 'This movie was terrible! I did not like it at all. I will never see a movie from this director again!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_review = text_preprocessing(review_test, word2int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
