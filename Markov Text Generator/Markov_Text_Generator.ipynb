{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Markov Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team members:** Guillem Amat (ga98), Sebastián Soriano Pérez (ss1072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sebastiannw/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Importing packages'''\n",
    "import numpy as np\n",
    "import re\n",
    "import pdb\n",
    "import nltk\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Markov Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_matrix(n: int, corpus: List[str]):\n",
    "    '''\n",
    "    Computes a matrix (numpy array) with the conditional probabilities of a word appearing after an n-gram as\n",
    "    determined by the corpus passed to this function. Returns the matrix (n_matrix), a dictionary of all the n-grams\n",
    "    found in the corpus and their assigned indexes (n_gram_dictionary), and a dictionary of all the words (or tokens)\n",
    "    found in the corpus and their assigned indexes (word_dictionary).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int \n",
    "        The length of n-grams.\n",
    "    corpus : list of strings \n",
    "        A source corpus (list of tokens).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    n_matrix : numpy array \n",
    "        Array of conditional probabilities of a word appearing after an n-gram, given that n-gram. The columns\n",
    "        correspond to unique words and the rows correspond to unique n-grams (as found in the corpus).\n",
    "    n_gram_dictionary : dict\n",
    "        Dictionary containing all unique n-grams found in the corpus as the keys. The values contain an index or int\n",
    "        identifier for the n-gram.\n",
    "    word_dictionary : dict\n",
    "        Dictionary containing all unique words (or tokens) in the corpus as the keys. The values contain an index or\n",
    "        int identifier for the words.\n",
    "        \n",
    "    '''\n",
    "    # Returns n-grams from a corpus\n",
    "    #pdb.set_trace()\n",
    "    n_gram_list = [tuple(corpus[i:i + n]) for i in range(len(corpus[:-n]))]\n",
    "    \n",
    "    # Creates unique n-gram indexes\n",
    "    distinct_n_grams  = list(set(n_gram_list))                                   # List of unique n-grams in corpus\n",
    "    n_gram_dictionary = {n_gram: i for i, n_gram in enumerate(distinct_n_grams)} # Stores n-gram's distinct_n_gram \n",
    "                                                                                 # index\n",
    "    \n",
    "    # Creates unique word indexes\n",
    "    distinct_words  = list(set(corpus))                                  # List of unique words in corpus\n",
    "    word_dictionary = {word: i for i, word in enumerate(distinct_words)} # Stores the word's distinct_words index\n",
    "        \n",
    "    # Creates an empty word-n-gram matrix to store the number of times each word follows an n-gram later on\n",
    "    n_count_matrix = np.zeros((len(distinct_n_grams), len(distinct_words)))\n",
    "    \n",
    "    # Loops through each n-gram in corpus to fill out n_gram_matrix\n",
    "    for i, n_gram in enumerate(n_gram_list[:-1]):\n",
    "        # Finds the n-gram's index in distinct_n_grams\n",
    "        n_gram_index = n_gram_dictionary[n_gram]\n",
    "        \n",
    "        # Finds the distinct_words's index of the word that follows the i-th n-gram (in the corpus)\n",
    "        word_index = word_dictionary[corpus[i + n]]\n",
    "        \n",
    "        # Adds +1 to the count of the n_gram_index, word_index value in n_gram_matrix (counts the number of times\n",
    "        # each word follows each n-gram as they appear in the corpus)\n",
    "        n_count_matrix[n_gram_index, word_index] += 1\n",
    "    \n",
    "    # Creates matrix of conditional probabilities of each word following an n-gram given that n_gram appears\n",
    "    n_matrix = n_count_matrix / np.sum(n_count_matrix, axis=1).reshape(-1, 1)\n",
    "    n_matrix = np.nan_to_num(n_matrix)\n",
    "    \n",
    "    return n_matrix, n_gram_dictionary, word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence: List[str], n: int, corpus: List[str], deterministic: bool =False) -> List[str]:\n",
    "    '''\n",
    "    Returns an extended sentence until the first ., ?, or ! is found OR until it has 15 total tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : list of strings \n",
    "        List of tokens that we’re trying to build on.\n",
    "    n : int \n",
    "        The length of n-grams to use for prediction.\n",
    "    corpus : list of strings \n",
    "        A source corpus (list of tokens).\n",
    "    deterministic : bool\n",
    "        A flag indicating whether the process should be deterministic.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    new_sentence : list of strings \n",
    "        Extended sentence built with this Markov Text Generator.\n",
    "    '''\n",
    "    # Creates n_matrix to be filled with conditional probabilities of a word following an n-gram given that n-gram\n",
    "    n_matrix          = {} # Dictionary with conditional probabilities\n",
    "    n_gram_dictionary = {} # Dictionary with n-grams in corpus\n",
    "    word_dictionary   = {} # Dictionary with words in corpus\n",
    "    \n",
    "    # Defines new_sentece to which new words will be appended, and the first n-gram from sentence\n",
    "    new_sentence = sentence.copy()      # Copy of sentence passed to finish_sentece, will be appended with new tokens\n",
    "    n_gram       = tuple(sentence[-n:]) # First n-gram found in the original sentence\n",
    "    \n",
    "    # Loops while new_sentence has 15 words (tokens) or less; Each iteration appends a new_word\n",
    "    while len(new_sentence) <= 15:\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        # Loops for i with values from n to 1 to find the word with the highest conditional probability of\n",
    "        # appearing after the last i-gram in the current iteration of new_sentence, as it appears in corpus\n",
    "        for i in range(n, 0, -1):\n",
    "            \n",
    "            # Checks if dictionary n_matrix has the matrix of contitional probabilities for the current i-gram value\n",
    "            if i not in n_matrix.keys(): \n",
    "                # Creates and stores dictionaries for the i value if it doesn't exist yet\n",
    "                n_matrix[i], n_gram_dictionary[i], word_dictionary[i] = compute_n_matrix(i, corpus)\n",
    "            \n",
    "            # Enters if the current i-gram (n_gram) appears in corpus (n_gram_dictionary[i])\n",
    "            if n_gram in n_gram_dictionary[i]:\n",
    "                # Obtains the index of the current i-gram in the current n_matrix[i]\n",
    "                n_gram_index = n_gram_dictionary[i][n_gram]\n",
    "                \n",
    "                # Retrieves the new_word to be appended to new_sentence as defined by input parameter 'deterministic'\n",
    "                if deterministic:\n",
    "                    # Retrieves the new_word with the maximum conditional probability of appearing after the i-gram\n",
    "                    # in corpus\n",
    "                    new_word = list(word_dictionary[i].keys())[np.argmax(n_matrix[i][n_gram_index, :])]\n",
    "                else:\n",
    "                    # Chooses a new_word randomly following the conditional probabilities defined in n_matrix[i]\n",
    "                    new_word = np.random.choice(list(word_dictionary[i].keys()), p=n_matrix[i][n_gram_index, :])\n",
    "                \n",
    "                # Breaks the loop of i from n to i if the current i-gram (n_gram) appeared in corpus\n",
    "                break\n",
    "            \n",
    "            # Gets here if current i-gram (n_gram) wasn't in corpus; Redefines it as an (i - 1)-gram (n_gram) \n",
    "            n_gram = tuple(new_sentence[-(i - 1):])\n",
    "            \n",
    "            # If the last i-gram (n_gram with n=1) wasn't found at the end of the loop, defines the new word to be ','\n",
    "            if i == 1: new_word = ','\n",
    "        \n",
    "        # Appends the new_word to new_sentence\n",
    "        new_sentence.append(new_word)\n",
    "        \n",
    "        # Breaks and stops appending to new_sentence if the last word (token) appended was '.', '?', or '!'\n",
    "        if re.match(r'^[.?!]$', new_word): break\n",
    "            \n",
    "        # Redefines the n_gram to be the last n-gram found in new_sentece (with its new appeneded word)\n",
    "        n_gram = tuple(new_sentence[-n:])\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Fixed variables for all test cases'''\n",
    "corpus = [w.lower() for w in nltk.corpus.gutenberg.words('austen-sense.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she',\n",
       " 'was',\n",
       " 'not',\n",
       " 'only',\n",
       " 'without',\n",
       " 'affection',\n",
       " 'for',\n",
       " 'the',\n",
       " 'person',\n",
       " 'who',\n",
       " 'was',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'remains',\n",
       " 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Test case 1'''\n",
    "sentence = ['she', 'was', 'not']\n",
    "n        = 3\n",
    "finish_sentence(sentence, n, corpus, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she',\n",
       " 'was',\n",
       " 'not',\n",
       " 'at',\n",
       " 'liberty',\n",
       " 'to',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'their',\n",
       " 'orders',\n",
       " ';',\n",
       " 'and',\n",
       " 'they',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'together']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Test case 2'''\n",
    "sentence = ['she', 'was', 'not']\n",
    "n        = 3\n",
    "finish_sentence(sentence, n, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
