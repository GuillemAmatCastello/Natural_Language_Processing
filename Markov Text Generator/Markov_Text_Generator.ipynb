{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Guillem Amat (ga98), Sebastian Soriano Perez (ss1072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from scipy.sparse import dok_matrix\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Return k-grams from a corpus\n",
    "n_gram_list = [tuple(corpus[i:i+n]) for i, _ in enumerate(corpus[:-n+1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Return k-grams count // Eliminate this step\n",
    "n_gram_count = {}\n",
    "for n_gram in n_gram_list:\n",
    "    n_gram_count[n_gram] = n_gram_count.get(n_gram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Find Word indexes\n",
    "word_dictionary = {word: i for i, word in enumerate(corpus)}\n",
    "distinct_words = len(set(corpus))\n",
    "distinct_n_grams = len(n_gram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Calculate length of vocabulary\n",
    "n_gram_indices = {n_gram: i for i, n_gram in enumerate(n_gram_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a word_matrix in which to store results\n",
    "n_gram_matrix = np.zeros((vocabulary, distinct_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 19 is out of bounds for axis 1 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-6728233fbdf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mn_gram_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_gram_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mn_gram_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_gram_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 19 is out of bounds for axis 1 with size 17"
     ]
    }
   ],
   "source": [
    "# Step 5: Loop to fill document term matrix\n",
    "for i, n_gram in enumerate(n_gram_list[:-n]):\n",
    "    n_gram_index = n_gram_indices[n_gram] \n",
    "    word_index = word_dictionary[corpus[i+n]]\n",
    "    n_gram_matrix[n_gram_index, word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make stochastic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Make deterministic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Implement Stupid Backoff for deterministic process(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step X: Calculate perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Markov Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "corpus = ['Hello', 'my', 'name', 'is', 'Guillem', '.',\n",
    "     'I', 'study', 'at', 'Duke', '.',\n",
    "     'I', 'am', 'working', 'on', 'this', '.',\n",
    "     'Hello', 'my', 'name', 'is', 'Sebastian', '.',\n",
    "     'I', 'am', 'your', 'teamate', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(corpus: str):\n",
    "    '''Cleans words on a list for text generation'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def k_grams(corpus: List[str], k: int) -> List[Tuple[str]]:\n",
    "#    '''Partitions the corpus into k-grams'''\n",
    "#    gram = [tuple(corpus[i:i+k]) for i, _ in enumerate(corpus[:-k+1])]\n",
    "#    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def k_grams(corpus: List[str], k: int) -> List:\n",
    "#    '''Partitions the corpus into k-grams'''\n",
    "#    k_gram = [' '.join(corpus[i:i+k]) for i, _ in enumerate(corpus[:-k+1])]\n",
    "#    return k_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_markov_generation(sentence, n, corpus) -> List[str]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_markov_generation(sentence, n, corpus) -> List[str]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence: List[str], n: int, corpus: List[str], deterministic: bool = False) -> List[str]:\n",
    "    '''Returns a completed sentence given its start'''\n",
    "    if deterministic is True:\n",
    "        return deterministic_markov_generation(sentence, n, corpus)\n",
    "    else:\n",
    "        return stochastic_markov_generation(sentence, n, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm Test Cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['she', 'was', 'not']\n",
    "corpus = [w.lower() for w in\n",
    "          nltk.corpus.gutenberg.words('austen-sense.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_sentence(sentence, 3, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
