{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Guillem Amat (ga98), Sebastián Soriano Pérez (ss1072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sebastiannw/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from scipy.sparse import dok_matrix\n",
    "import re\n",
    "import pdb\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Markov Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_matrix(n: int, corpus: List[str]):\n",
    "    '''\n",
    "    Computes a matrix (numpy array) with the conditional probabilities of a word appearing after an n-gram as\n",
    "    determined by the corpus passed to this function. Returns the matrix (n_matrix), a dictionary of all the\n",
    "    n-grams found in the corpus and their assigned indexes (n_gram_dictionary), and a dictionary of all the\n",
    "    words (or tokens) found in the corpus and their assigned indexes (word_dictionary).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int \n",
    "        The length of n-grams.\n",
    "    corpus : list of strings \n",
    "        A source corpus (list of tokens).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    n_matrix : numpy array \n",
    "        Array of conditional probabilities of a word appearing after an n-gram, given that n-gram. The columns\n",
    "        correspond to unique words and the rows correspond to unique n-grams (as found in the corpus).\n",
    "    n_gram_dictionary : dict\n",
    "        Dictionary containing all unique n-grams found in the corpus as the keys. The values contain an index\n",
    "        or int identifier for the n-gram.\n",
    "    word_dictionary : dict\n",
    "        Dictionary containing all unique words (or tokens) in the corpus as the keys. The values contain an\n",
    "        index or int identifier for the words.\n",
    "        \n",
    "    '''\n",
    "    # Returns n-grams from a corpus\n",
    "    #pdb.set_trace()\n",
    "    n_gram_list = [tuple(corpus[i:i + n]) for i in range(len(corpus[:-n]))]\n",
    "    \n",
    "    # Creates unique n-gram indexes\n",
    "    distinct_n_grams  = list(set(n_gram_list))                                   # List of unique n-grams in corpus\n",
    "    n_gram_dictionary = {n_gram: i for i, n_gram in enumerate(distinct_n_grams)} # Stores n-gram's distinct_n_gram index\n",
    "    \n",
    "    # Creates unique word indexes\n",
    "    distinct_words  = list(set(corpus))                                  # List of unique words in corpus\n",
    "    word_dictionary = {word: i for i, word in enumerate(distinct_words)} # Stores the word's distinct_words index\n",
    "        \n",
    "    # Creates an empty word-n-gram matrix to store the number of times each word follows an n-gram later on\n",
    "    n_count_matrix = np.zeros((len(distinct_n_grams), len(distinct_words)))\n",
    "    \n",
    "    # Loops through each n-gram in corpus to fill out n_gram_matrix\n",
    "    for i, n_gram in enumerate(n_gram_list[:-1]):\n",
    "        # Finds the n-gram's index in distinct_n_grams\n",
    "        n_gram_index = n_gram_dictionary[n_gram]\n",
    "        \n",
    "        # Finds the distinct_words's index of the word that follows the i-th n-gram (in the corpus)\n",
    "        word_index = word_dictionary[corpus[i + n]]\n",
    "        \n",
    "        # Adds +1 to the count of the n_gram_index, word_index value in n_gram_matrix\n",
    "        # (counts the number of times each word follows each n-gram as they appear in the corpus)\n",
    "        n_count_matrix[n_gram_index, word_index] += 1\n",
    "    \n",
    "    # Creates matrix of conditional probabilities of each word following an n-gram given that n_gram appears\n",
    "    n_matrix = n_count_matrix / np.sum(n_count_matrix, axis=1).reshape(-1, 1)\n",
    "    n_matrix = np.nan_to_num(n_matrix)\n",
    "    \n",
    "    return n_matrix, n_gram_dictionary, word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence: List[str], n: int, corpus: List[str], deterministic: bool =False) -> List[str]:\n",
    "    '''\n",
    "    Returns an extended sentence until the first ., ?, or ! is found OR until it has 15 total tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : list of strings \n",
    "        List of tokens that we’re trying to build on.\n",
    "    n : int \n",
    "        The length of n-grams to use for prediction.\n",
    "    corpus : list of strings \n",
    "        A source corpus (list of tokens).\n",
    "    deterministic : bool\n",
    "        A flag indicating whether the process should be deterministic.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    complete_sentence : list of strings \n",
    "        Extended sentence built with this Markov Text Generator.\n",
    "    '''\n",
    "    # Creates n_matrix to be filled with conditional probabilities of a word following an n-gram given that n-gram\n",
    "    n_matrix          = {} # Dictionary with conditional probabilities\n",
    "    n_gram_dictionary = {} # Dictionary with n-grams in corpus\n",
    "    word_dictionary   = {} # Dictionary with words in corpus\n",
    "    \n",
    "    new_sentence = sentence.copy()      # Copy of sentence passed to finish_sentece, will be appended with new tokens\n",
    "    n_gram       = tuple(sentence[-n:]) # First n-gram found in the original sentence\n",
    "    \n",
    "    while len(new_sentence) <= 15:\n",
    "        #pdb.set_trace()\n",
    "        for i in range(n, 0, -1):\n",
    "            \n",
    "            if i not in n_matrix.keys(): \n",
    "                # Stores dictionaries for the i value\n",
    "                n_matrix[i], n_gram_dictionary[i], word_dictionary[i] = compute_n_matrix(i, corpus)\n",
    "            \n",
    "            if n_gram in n_gram_dictionary[i]:\n",
    "                n_gram_index = n_gram_dictionary[i][n_gram]\n",
    "\n",
    "                if deterministic:\n",
    "                    new_word = list(word_dictionary[i].keys())[np.argmax(n_matrix[i][n_gram_index, :])]\n",
    "                else:\n",
    "                    new_word = np.random.choice(list(word_dictionary[i].keys()), p=n_matrix[i][n_gram_index, :])\n",
    "                \n",
    "                break\n",
    "            \n",
    "            n_gram = tuple(new_sentence[-(i - 1):])\n",
    "            \n",
    "            if i == 1: new_word = ','\n",
    "\n",
    "        new_sentence.append(new_word)\n",
    "        n_gram = tuple(new_sentence[-n:])\n",
    "        \n",
    "        if re.match(r'^[.?!]$', new_word): break\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if re.match(r'^[.?!]$', '.'): print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dessy',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'large',\n",
       " 'one',\n",
       " ',',\n",
       " 'i',\n",
       " 'know',\n",
       " ',',\n",
       " 'if',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'were',\n",
       " 'diminished',\n",
       " 'one']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_sentence(sentence, n, corpus, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm Test Cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['she', 'was', 'not']\n",
    "n        = 3\n",
    "corpus   = [w.lower() for w in nltk.corpus.gutenberg.words('austen-sense.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finish_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ee58d28adc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinish_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'finish_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "finish_sentence(sentence, 3, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
