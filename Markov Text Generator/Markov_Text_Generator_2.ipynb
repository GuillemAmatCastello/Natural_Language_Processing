{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Guillem Amat (ga98), Sebastian Soriano Perez (ss1072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from scipy.sparse import dok_matrix\n",
    "import re\n",
    "import pdb\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.01, 0.01, 0.98, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_choice = []\n",
    "\n",
    "for i in range(10):\n",
    "    list_choice.append(np.random.choice(p = np.tolist(np.array(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.31033974,  0.17671456, -0.04065318, -0.09398891],\n",
       "       [-0.90297059, -0.89339129,  1.06367668,  2.0617725 ],\n",
       "       [ 0.57125854, -0.03336481, -1.23787733,  1.46174133]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.random.randn(3,4)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57125854, -0.03336481, -1.23787733,  1.46174133])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "corpus = ['Hello', 'my', 'name', 'is', 'Guillem', '.',\n",
    "          'I', 'study', 'at', 'Duke', '.',\n",
    "          'I', 'am', 'working', 'on', 'this', '.',\n",
    "          'Hello', 'my', 'name', 'is', 'Sebastian', '.',\n",
    "          'I', 'am', 'your', 'teamate', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Return k-grams from a corpus\n",
    "n_gram_list = [tuple(corpus[i:i+n]) for i, _ in enumerate(corpus[:-n+1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Find Word indexes\n",
    "distinct_words = list(set(corpus))\n",
    "word_dictionary = {word: i for i, word in enumerate(distinct_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Calculate length of vocabulary\n",
    "distinct_n_grams = list(set(n_gram_list))\n",
    "n_gram_dictionary = {n_gram: i for i, n_gram in enumerate(distinct_n_grams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a word_matrix in which to store results\n",
    "n_gram_matrix = np.zeros((len(distinct_n_grams), len(distinct_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Loop to fill document term matrix\n",
    "for i, n_gram in enumerate(n_gram_list[:-1]):\n",
    "    #pdb.set_trace()\n",
    "    n_gram_index = n_gram_dictionary[n_gram] \n",
    "    word_index = word_dictionary[corpus[i+n]]\n",
    "    n_gram_matrix[n_gram_index, word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107093, 6403)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "probabilities = n_gram_matrix/np.sum(n_gram_matrix, axis = 1).reshape(-1,1)\n",
    "probabilities = np.nan_to_num(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make stochastic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Make deterministic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Implement Stupid Backoff for deterministic process(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step X: Calculate perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Markov Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_matrix(n, corpus):\n",
    "    '''Creates the state matrix'''\n",
    "    n_gram_list = [tuple(corpus[i:i+n]) for i, _ in enumerate(corpus[:-n+1])]\n",
    "    \n",
    "    distinct_words = list(set(corpus))\n",
    "    word_dictionary = {word: i for i, word in enumerate(distinct_words)}\n",
    "    \n",
    "    distinct_n_grams = list(set(n_gram_list))\n",
    "    n_gram_dictionary = {n_gram: i for i, n_gram in enumerate(distinct_n_grams)}\n",
    "    \n",
    "     \n",
    "    n_gram_matrix = np.zeros((len(distinct_n_grams), len(distinct_words)))\n",
    "    \n",
    "    for i, n_gram in enumerate(n_gram_list[:-1]):\n",
    "        n_gram_index = n_gram_dictionary[n_gram] \n",
    "        word_index = word_dictionary[corpus[i+n]]\n",
    "        n_gram_matrix[n_gram_index, word_index] += 1\n",
    "        \n",
    "    probabilities = n_gram_matrix/np.sum(n_gram_matrix, axis = 1).reshape(-1,1)\n",
    "    probabilities = np.nan_to_num(probabilities)\n",
    "    \n",
    "    return [probabilities, n_gram_dictionary, word_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_markov_generation(sentence, n, corpus) -> List[str]:\n",
    "    states_matrix\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_markov_generation(sentence, n, corpus) -> List[str]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence: List[str], n: int, corpus: List[str], deterministic: bool = False) -> List[str]:\n",
    "    '''Returns a completed sentence given its start'''\n",
    "    initial_n_gram = \n",
    "    matrix_dictionary[n] = states_matrix(n, corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tuple(corpus[i:i+n]) for i, _ in enumerate(sentence[:-n+1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm Test Cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['she', 'was', 'not']; n=3\n",
    "corpus = [w.lower() for w in\n",
    "          nltk.corpus.gutenberg.words('austen-sense.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_sentence(sentence, 3, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
